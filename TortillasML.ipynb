{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3 Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üí° Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the relevant columns into a new DataFrame for correlation analysis\n",
    "correlation_df = finalDf[['Total guests', 'PSV_Count', 'Effenaar_Count', 'Temperature', 'Rain', 'Duration rain', 'Max rain', 'Wind']]\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = correlation_df.corr()\n",
    "\n",
    "# Generate a heatmap of the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Map')\n",
    "plt.show()\n",
    "# Encode the 'Day' column into numerical values\n",
    "day_encoded = finalDf['Day'].cat.codes\n",
    "\n",
    "# Calculate the correlation between 'Day' and 'Total guests'\n",
    "day_guests_correlation = day_encoded.corr(finalDf['Total guests'])\n",
    "\n",
    "print(\"Correlation between Day and Total guests:\", day_guests_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of now, the most important features are the type of day, with Effenaar events showing the highest correlation. It's important to note that these observations may change over time, but for testing purposes, this setup suffices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variables\n",
    "features = [\"Day\", \"Effenaar_Count\"]\n",
    "target = \"Total guests\"\n",
    "\n",
    "X = finalDf[features]\n",
    "y = finalDf[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü™ì Splitting into train/test\n",
    "Before the model can be trained, a little part of the data is to be put aside for testing purposes. The reasoning here is that the model trains with, for example 80% of the data available, and the other 20% is used to ask it to predict the target variable for. Because the true target variable of that 20% is known, we can compare the predictions with the ground truth and devise how well the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n",
    "print(\"There are in total\", len(X), \"observations, of which\", len(X_train), \"are now in the train set, and\", len(X_test), \"in the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚öñÔ∏è Scaling\n",
    "For other machine learning algorithms scaling may be needed, however, linear regression can usually do fine without scaling because it will make a mathematically formula to predict the target with, that can adapt to features in different units. However, for visualization purposes it may be required to scale anyway, or plots may look bad. For now, no scaling is applied. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üÜî Encoding\n",
    "Given the fact that machine learning algorithms work with only numeric values, often the input data needs to be encoded, which means turning the non-numeric data into numeric representations (codes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded = pd.get_dummies(X_train, columns=['Day'], drop_first=True)\n",
    "X_test_encoded = pd.get_dummies(X_test, columns=['Day'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß¨ Modelling\n",
    "In this step only the train set is used to fit the model, which in this case uses a Linear Regression algorithm named [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html). And after that the test set is used to calculate the model's score, in other words how well it performs. For regression problems the score is provided as the [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination) denoted *R¬≤*, which is a fraction where any value closer to 1 is considered better, and 1 itself (100% accurate) is usually impossible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "result = model.fit(X_train_encoded, y_train)\n",
    "score = model.score(X_test_encoded, y_test)\n",
    "print(\"R¬≤:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¨ Evaluation\n",
    "Now, for each in the test set the model makes a prediction of the total guests. Since the true total guest is known, it is then possible to compare the truth with the prediction and calculate an error from that, meaning *\"how far away is the prediction from the truth?\"*. Note that the error is absolute (non-negative), and in this example it is also cast to an integer for legibility reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Make predictions using the trained model on the test data\n",
    "predictions = model.predict(X_test_encoded)\n",
    "\n",
    "# Step 2: Create a DataFrame to store the true total guests, predicted total guests, and the error\n",
    "prediction_overview = pd.DataFrame()\n",
    "prediction_overview[\"truth\"] = y_test\n",
    "prediction_overview[\"prediction\"] = predictions\n",
    "\n",
    "# Step 3: Calculate the absolute error\n",
    "prediction_overview[\"error\"] = prediction_overview[\"truth\"] - prediction_overview[\"prediction\"]\n",
    "prediction_overview[\"error\"] = abs(prediction_overview[\"error\"]).astype(int)\n",
    "\n",
    "# Step 4: Reset the index of the DataFrame\n",
    "prediction_overview = prediction_overview.reset_index(drop=True)\n",
    "\n",
    "# Display the prediction overview DataFrame\n",
    "print(prediction_overview)\n",
    "plot = sns.regplot(y=y_test.values.flatten(), x=predictions.flatten(), line_kws={\"color\": \"r\"})\n",
    "plot.set_xlabel(\"predicted amount\")\n",
    "plot.set_ylabel(\"true amount\")\n",
    "plot\n",
    "from sklearn.metrics import max_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "me = max_error(y_test, predictions)\n",
    "me = math.ceil(me)\n",
    "print(\"Max Error:\", me)\n",
    "\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = math.sqrt(mse)\n",
    "rmse = math.ceil(rmse)\n",
    "print(\"Root Mean Squared Error:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üó≥Ô∏è Conclusion\n",
    "The first model, even though it was quite basic and used only a few factors, showed some good results. It predicted about 77% of the variation in the number of guests accurately, which is a decent start. It means that the type of day and the number of events at Effenaar have a noticeable effect on how many guests show up.\n",
    "\n",
    "To make our predictions even better, new things have to be tried. First, more factors could be addded to the model, like weather conditions or special occasions happening nearby. Also, getting more data would help. The more information our model has, the smarter it gets.\n",
    "\n",
    "Trying different ways of making predictions is also important. The first model was pretty simple, but there are other methods out there that might work better for the data.\n",
    "\n",
    "So, while the first model did okay, there's still lots of room to make it better. By adding more factors, getting more data, and trying different methods, we hope to build a model that can predict the number of guests even more accurately in the future."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
